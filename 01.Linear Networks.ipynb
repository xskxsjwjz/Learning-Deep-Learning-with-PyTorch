{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2121b3d",
   "metadata": {},
   "source": [
    "## 01.线性神经网络\n",
    "\n",
    "### 1-1：线性回归\n",
    "\n",
    "#### **预测值定义**\n",
    "\n",
    "正如高中学过的最小二乘法，线性回归的目标是拟合一条直线，使得损失函数最小化。线性回归的数学表达式为：\n",
    "$$\n",
    "y = w_1 x_1 + w_2 x_2 + \\cdots + w_d x_d + b\n",
    "$$\n",
    "\n",
    "其中，$y$ 是预测值，$x_1, x_2, \\ldots, x_d$ 是输入特征，$w_1, w_2, \\ldots, w_d$ 是对应的权重，$b$ 是偏置项。  \n",
    "为了方便，我们用向量形式表示上述公式：\n",
    "$$\n",
    "y = \\mathbf{w}^T \\mathbf{x} + b\n",
    "$$\n",
    "其中，$\\mathbf{w} = [w_1, w_2, \\ldots, w_d]^T$ 是权重向量，$\\mathbf{x} = [x_1, x_2, \\ldots, x_d]^T$ 是输入特征向量。\n",
    "\n",
    "#### **损失函数**\n",
    "\n",
    "线性回归采用 **均方误差（MSE）** 作为损失函数，定义如下：\n",
    "$$\n",
    "L(\\mathbf{w}, b) = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{2} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "其中，$N$ 是样本数量，$y_i$ 是第 $i$ 个样本的真实值，$\\hat{y}_i$ 是第 $i$ 个样本的预测值。\n",
    "\n",
    "#### **解析解**\n",
    "\n",
    "由线性代数知识得到线性回归的解析解如下：\n",
    "$$\\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$$\n",
    "$$b = \\bar{y} - \\mathbf{w}^T \\bar{\\mathbf{x}}$$\n",
    "其中，$\\mathbf{X}$ 是输入特征矩阵，$\\mathbf{y}$ 是目标值向量，$\\bar{y}$ 是目标值的平均值，$\\bar{\\mathbf{x}}$ 是输入特征的平均值。\n",
    "\n",
    "然而实际上数据一般会有噪声，导致 $\\mathbf{X}^T \\mathbf{X}$ 不可逆，因此直接求解析解虽然快而简单，但并不实用。下面介绍梯度下降法，通过逐步迭代求解。\n",
    "\n",
    "#### **梯度下降法**\n",
    "\n",
    "我们将现有数据和特征代入损失函数，得到如下表达式：\n",
    "$$L(\\mathbf{w}, b) = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{2} (y_i - (\\mathbf{w}^T \\mathbf{x}^{(i)} + b))^2$$\n",
    "其中 $\\mathbf{x}^{(i)}$ 是第 $i$ 个样本的输入特征向量。  \n",
    "梯度方向就是损失函数下降最快的方向，因此接下来我们将计算损失函数关于权重 $\\mathbf{w}$ 和偏置 $b$ 的梯度。\n",
    "\n",
    "梯度的计算如下：\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{w}} = -\\frac{1}{N} \\sum_{i=1}^{N} (y_i - (\\mathbf{w}^T \\mathbf{x}^{(i)} + b)) \\mathbf{x}^{(i)}$$\n",
    "$$\\frac{\\partial L}{\\partial b} = -\\frac{1}{N} \\sum_{i=1}^{N} (y_i - (\\mathbf{w}^T \\mathbf{x}^{(i)} + b))$$\n",
    "\n",
    "我们采用以下的式子来更新权重和偏置：\n",
    "$$\\mathbf{w} := \\mathbf{w} - \\alpha \\frac{\\partial L}{\\partial \\mathbf{w}}$$\n",
    "$$b := b - \\alpha \\frac{\\partial L}{\\partial b}$$\n",
    "其中，$\\alpha$ 是学习率，是一个超参数，控制每次更新的步长大小。如果 $\\alpha$ 太大，可能会导致损失函数震荡甚至发散；如果 $\\alpha$ 太小，收敛速度会非常慢。\n",
    "\n",
    "#### **预测**\n",
    "\n",
    "在训练完成后，我们可以使用以下公式进行预测：\n",
    "$$\\hat{y} = \\mathbf{w}^T \\mathbf{x} + b$$\n",
    "其中，$\\hat{y}$ 是预测值，$\\mathbf{x}$ 是输入特征向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e90a7d",
   "metadata": {},
   "source": [
    "### 代码实操\n",
    "\n",
    "下面我们使用 PyTorch 随机生成数据进行线性回归测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bbac0f",
   "metadata": {},
   "source": [
    "接下来我们定义线性回归需要的代码。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
